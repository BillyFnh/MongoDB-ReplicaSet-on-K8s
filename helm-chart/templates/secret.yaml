---
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-auth
data:
  password: {{.Values.db.auth.password | b64enc}}
  keyfile: {{.Values.db.auth.keyfile | b64enc}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter-config
  namespace: monitoring
data:
  prometheus_adapter.yml: |
    rules:
        - seriesQuery: 'myapphttp_process_open_fds'
          resources:
            template: "<<.Resource>>"
          name:
            matches: "^(.*)"
            as: "${1}"
          metricsQuery: 'max(<<.Series>>) by (job)'
        - seriesQuery: 'myapphttp_process_cpu_seconds_total'
          resources:
            template: "<<.Resource>>"
          name:
            matches: "^(.*)"
            as: "${1}"
          metricsQuery: 'max(<<.Series>>) by (job)'
        - seriesQuery: 'myapphttp_established_connections'
          resources:
            template: "<<.Resource>>"
          name:
            matches: "^(.*)"
            as: "${1}"
          metricsQuery: 'max(<<.Series>>) by (job)'
        - seriesQuery: 'myapphttp_load1'
          resources:
            template: "<<.Resource>>"
          name:
            matches: "^(.*)"
            as: "${1}"
          metricsQuery: 'max(<<.Series>>) by (job)'
---
apiVersion: v1
kind: Secret
metadata:
  name: prometheus-secrets
  namespace: monitoring
data:
  prometheus.yaml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - 1cms-monitoring.cern.ch:30093
    rule_files:
      - "/etc/prometheus/k8s.rules"
    remote_write:
      - url: http://1cms-monitoring-ha1.cern.ch:30428/api/v1/write
        queue_config:
          max_samples_per_send: 10000
          max_shards: 30
      - url: http://1cms-monitoring-ha2.cern.ch:30428/api/v1/write
        queue_config:
          max_samples_per_send: 10000
          max_shards: 30
    scrape_configs:
      - job_name: "kube-eagle"
        static_configs:
            - targets: ["kube-eagle.default.svc.cluster.local:8080"]
      - job_name: "nginx-ingress-controller"
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - kube-system
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          action: keep
          regex: ^(10254)
      - job_name: "kubernetes-apiservers"
        kubernetes_sd_configs:
        - role: pod
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
      - job_name: "kubernetes-nodes"
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
      - job_name: "kubernetes-pods"
        kubernetes_sd_configs:
        - role: pod
        # for more information about prometheus relabeling see
        # https://blog.freshtracks.io/prometheus-relabel-rules-and-the-action-parameter-39c71959354a
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: host
        - action: labeldrop
          regex: __meta_kubernetes_pod_label_pod_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: ns
        - action: labeldrop
          regex: __meta_kubernetes_namespace(.+)
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: apod
      - job_name: "kubernetes-cadvisor"
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
  k8s.rules: |
    groups:
    - name: k8s
      rules:

      - alert: KubernetesNodeReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
          description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesJobFailed
        expr: kube_job_status_failed > 0
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
          description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesCronjobSuspended
        expr: kube_cronjob_spec_suspend != 0
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
          description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPersistentvolumeclaimPending
        expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
          description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesVolumeOutOfDiskSpace
        expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})"
          description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesVolumeFullInFourDays
        expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes Volume full in four days (instance {{ $labels.instance }})"
          description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPersistentvolumeError
        expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})"
          description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesHpaScalingAbility
        expr: kube_hpa_status_condition{condition="false", status="AbleToScale"} == 1
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes HPA scaling ability (instance {{ $labels.instance }})"
          description: "Pod is unable to scale\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    #  - alert: KubernetesPodNotHealthy
    #    expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[1h:]) > 0
    #    for: 5m
    #    labels:
    #      severity: critical
    #      tag: k8s
    #      kind: cluster
    #    annotations:
    #      summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
    #      description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 5
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesApiServerErrors
        expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[2m])) / sum(rate(apiserver_request_count{job="apiserver"}[2m])) * 100 > 3
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
          description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesApiClientErrors
        expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[2m])) by (instance, job) / sum(rate(rest_client_requests_total[2m])) by (instance, job)) * 100 > 1
        for: 5m
        labels:
          severity: critical
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
          description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesApiServerLatency
        expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket{verb!~"CONNECT|WATCHLIST|WATCH|PROXY"}) WITHOUT (instance, resource)) / 1e+06 > 1
        for: 5m
        labels:
          severity: warning
          tag: k8s
          kind: cluster
        annotations:
          summary: "Kubernetes API server latency (instance {{ $labels.instance }})"
          description: "Kubernetes API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
---
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: logstash
#   namespace: monitoring
# data:
#   logstash.conf: |-
#     input {
#       beats {
#         port => 5044
#       }
#     }

#     filter {
#       ruby {
#         code => "event.set('timestamp',(event.get('@timestamp').to_f * 1000).to_i)"
#       }
#       mutate { replace => { "producer" => "cmsweb" } }

#       # https://github.com/elastic/logstash/blob/v1.4.2/patterns/grok-patterns

#       # capture apache records
#       # example of cmsweb apache entry
#       #[10/Mar/2019:00:59:59 +0100] cmsweb.cern.ch 137.138.152.31 "GET /reqmgr2/data/request?name=vlimant_ACDC0_task_B2G-RunIIFall17wmLHEGS-00607__v1_T_190304_194213_4399 HTTP/1.1" 200 [data: 3044 in 29413 out 11256 body 70659 us ] [auth: TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "/DC=ch/DC=cern/OU=computers/CN=wmagent/vocms0308.cern.ch" "-" ] [ref: "-" "WMCore.Services.Requests/v001" ]

#       grok {
#         match => { "message" => '\[%{HTTPDATE:tstamp}\] %{DATA:frontend} %{IPORHOST:clientip} "%{WORD:method} %{NOTSPACE:request} %{DATA:httpversion}" %{NUMBER:code:int} \[data:.*\] \[auth: %{DATA:tls} %{DATA:crypto} "%{DATA:dn}".*\] \[ref: "%{DATA}.*" "%{DATA:client}" \]' }
#       }
#       grok {
#          match => { "request" => '/%{WORD:system}%{UNIXPATH:uri_path}%{URIPARAM:uri_params}?' }
#       }
#       if [uri_params] {
#           grok {
#              match => { "uri_path" => '/.*/%{DATA:api}$' }
#           }
#           if [api] == "" {
#               grok {
#                  match => { "uri_path" => '/.*/%{DATA:api}/$' }
#               }
#           }
#       } else {
#           grok {
#              match => { "request" => '/.*/%{DATA:api}$' }
#           }
#           if [api] == "" {
#               grok {
#                  match => { "request" => '/.*/%{DATA:api}/$' }
#               }
#           }
#           # mutate { replace => { "api" => "" } }
#       }
#       if [uri_params] and ![api] {
#           grok {
#              match => { "uri_path" => '/.*/%{DATA:api}/$' }
#           }
#       }
#       if ![api] {
#           mutate { replace => { "api" => "%{request}" } }
#           mutate { replace => { "system" => "%{request}" } }
#       }
#       # https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html
#       # date string example: [10/Mar/2019:00:59:59 +0100]
#       date {
#          match => [ "tstamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
#          target => "date_object"
#       }
#       ruby {
#          code => "event.set('rec_timestamp',event.get('date_object').to_i)
#                   event.set('rec_date',event.get('date_object'))
#                  "
#       }

#       #Remove the numeric CNs from the client dn.
#       mutate {
#         gsub =>  [
#             "dn","/CN=\d+",""
#         ]
#       }
#     }

#     # send results (JSON records) to local file
#     output {
#       file {
#           path => "/tmp/logstash-output.log"
#       }
#     }

#     # send results (JSON records) to CERN MONIT HTTP endpoint
#     #output {
#     #    if [type] == "apache" {
#     #        http {
#     #            http_method => post
#     #            url => "http://monit-logs.cern.ch:10012/"
#     #            format => "message"
#     #            content_type => "application/json; charset=UTF-8"
#     #            message => '[{"producer": "%{producer}","type": "%{type}","method":"%{method}","code":"%{code}","client":"%{client}","api":"%{api}","clientip":"%{clientip}","dn":"%{dn}","system":"%{system}","uri_path":"%{uri_path}","host":"%{host}","frontend":"%{frontend}","timestamp":"%{timestamp}","path":"%{path}","rec_timestamp":"%{rec_timestamp}","rec_date":"%{rec_date}"}]'
#     #        }
#     #    }
#     #}
#   logstash.yml: |-
#     http.host: "0.0.0.0"
#     path.config: /usr/share/logstash/pipeline
#     xpack.monitoring.enabled: false

